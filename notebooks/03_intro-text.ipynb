{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Robot Judge\n",
    "### 3. Text Data Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "# set this to your working directory\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv('death-penalty-cases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32567 entries, 0 to 32566\n",
      "Data columns (total 7 columns):\n",
      "court_id     32567 non-null object\n",
      "author_id    18215 non-null float64\n",
      "state        32567 non-null object\n",
      "year         32567 non-null int64\n",
      "dateFiled    32567 non-null object\n",
      "citeCount    32567 non-null int64\n",
      "snippet      32567 non-null object\n",
      "dtypes: float64(1), int64(2), object(4)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df1.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "texapp             2577\n",
       "texcrimapp         2380\n",
       "fla                1927\n",
       "cal                1310\n",
       "ga                 1104\n",
       "illappct           1077\n",
       "pa                  930\n",
       "miss                925\n",
       "ill                 895\n",
       "oklacrimapp         760\n",
       "nc                  731\n",
       "calctapp            723\n",
       "tenncrimapp         723\n",
       "alacrimapp          711\n",
       "ohioctapp           611\n",
       "ariz                552\n",
       "wva                 545\n",
       "ind                 515\n",
       "la                  512\n",
       "mo                  507\n",
       "ark                 459\n",
       "nysd                390\n",
       "ala                 379\n",
       "tenn                368\n",
       "fladistctapp        359\n",
       "nyed                325\n",
       "sc                  323\n",
       "nev                 294\n",
       "pasuperct           291\n",
       "wash                272\n",
       "                   ... \n",
       "iasd                  7\n",
       "akd                   6\n",
       "wyd                   6\n",
       "connsuperct           6\n",
       "lamd                  6\n",
       "vid                   5\n",
       "oked                  5\n",
       "oklaag                5\n",
       "wiwd                  4\n",
       "ilsd                  4\n",
       "oknd                  3\n",
       "mdag                  3\n",
       "ndd                   3\n",
       "arkag                 3\n",
       "nyappdiv              3\n",
       "moag                  3\n",
       "nebctapp              3\n",
       "laag                  3\n",
       "ohioctcl              3\n",
       "hawapp                2\n",
       "risuperct             2\n",
       "texreview             1\n",
       "arwd                  1\n",
       "coloag                1\n",
       "oklacivapp            1\n",
       "southcarolinaed       1\n",
       "nmid                  1\n",
       "kanag                 1\n",
       "njch                  1\n",
       "alacivapp             1\n",
       "Name: court_id, Length: 206, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['court_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f9b29f9ae10>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f9b29cd4080>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[['year','citeCount']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Iterating over documents in a dataframe\n",
    "###################################\n",
    "from txt_utils import process_document\n",
    "\n",
    "processed = {} # empty python dictionary for processed data\n",
    "# iterate over rows\n",
    "for i, row in df1.iterrows():\n",
    "    docid = i # make document identifier\n",
    "    text = row['snippet']     # get text snippet\n",
    "    document = process_document(text) # get sentences/tokens\n",
    "    processed[docid] = document # add to dictionary    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Iterating over documents in text files\n",
    "###################################\n",
    "# select all files in your directory\n",
    "from glob import glob\n",
    "fnames = glob('contracts/*txt') # selects files using wildcards\n",
    "\n",
    "# iterate over files\n",
    "for fname in fnames:\n",
    "    docid = fname.split('/')[-1][:-4] # get docid from filename\n",
    "    text = open(fname).read() # read file as string\n",
    "    document = process_document(text) # get sentences/tokens\n",
    "    processed[docid] = document # add to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Saving data in python\n",
    "###################################\n",
    "# save as python pickle\n",
    "pd.to_pickle(processed, 'processed_corpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Data-frames\n",
    "#df_merged = pd.merge(df1,df2,on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n  <meta charset=\"utf-8\"/>\\n  <meta http-equiv=\"Content-Language\" content=\"en\"/>\\n  <meta name=\"language\" content=\"en_us\"/>\\n  <meta name=\"viewport\" content=\"width=device-width,initial-scale=1\"/>\\n\\n  \\n  <meta name=\"description\" content=\"Opinion for People v. Germany, 674 P.2d 345 \\xe2\\x80\\x94 Brought to you by Free Law Project, a non-profit dedicated to creating high qual'\n",
      "b'ik.php?idsite=1\"\\n                    style=\"border:0;\" alt=\"\"/></p></noscript>\\n  <!-- End Piwik Code -->\\n\\n<!--[if lt IE 10 ]>\\n<script src=\"//ajax.googleapis.com/ajax/libs/chrome-frame/1.0.3/CFInstall.min.js\"></script>\\n<script>window.attachEvent(\\'onload\\',function(){CFInstall.check({mode:\\'inline\\', url:\\'/bad-browser/\\', cssText: \\'width: 100%; height: 200px;\\' })})</script>\\n<![endif]-->\\n</body>\\n</html>\\n'\n",
      "78389\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# Screen Scraping\n",
    "###################################\n",
    "\n",
    "import urllib # Python's module for accessing web pages\n",
    "url = 'https://goo.gl/VRF8Xs' # shortened URL for court case\n",
    "page = urllib.request.urlopen(url) # open the web page\n",
    "\n",
    "html = page.read() # read web page contents as a string\n",
    "print(html[:400])  # print first 400 characters\n",
    "print(html[-400:]) # print last 400 characters\n",
    "print(len(html))   # print length of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>People v. Germany, 674 P.2d 345 – CourtListener.com</title>\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# HTML parsing\n",
    "###################################\n",
    "\n",
    "# Parse raw HTML\n",
    "from bs4 import BeautifulSoup # package for parsing HTML\n",
    "soup = BeautifulSoup(html, 'lxml') # parse html of web page\n",
    "print(soup.title) # example usage: print title item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546\n"
     ]
    }
   ],
   "source": [
    "# extract text\n",
    "text = soup.get_text() # get text (remove HTML markup)\n",
    "lines = text.splitlines() # split string into separate lines\n",
    "print(len(lines)) # print number of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "['People v. Germany, 674 P.2d 345 – CourtListener.com', 'Toggle navigation', 'About', 'FAQ', 'Tour', 'Donate', 'Sign in / Register', 'From Free Law Project, a 501(c)(3) non-profit.', 'Opinions\\xa0', 'Advanced Search', 'Citation Look Up', 'RECAP Archive', 'Oral Arguments', 'Judges', 'Visualizations\\xa0', 'Gallery', 'SCOTUS Networks', 'New Network', '\\xa0Donate', 'Your Notes']\n"
     ]
    }
   ],
   "source": [
    "lines = [line for line in lines if line != ''] # drop empty lines\n",
    "print(len(lines)) # print number of lines\n",
    "print(lines[:20]) # print first 20 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizations \n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# Removing unicode characters\n",
    "###################################\n",
    "\n",
    "from unidecode import unidecode # package for removing unicode\n",
    "fixed = unidecode('Visualizations\\xa0') # example usage\n",
    "print(fixed) # print cleaned string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8fe46d12bfdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogletrans\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'이 문장은 한글로 쓰여졌습니다.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# actual source language that will be recognized by Google Translator when the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_acquirer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         params = utils.build_params(query=text, src=src, dest=dest,\n\u001b[1;32m     77\u001b[0m                                     token=token)\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/googletrans/gtoken.py\u001b[0m in \u001b[0;36mdo\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/googletrans/gtoken.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# this will be the same as python code after stripping out a reserved word 'var'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRE_TKK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'var '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;31m# unescape special ascii characters such like a \\x3d(=)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "#############\n",
    "# Translation\n",
    "#############\n",
    "\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "lang = translator.detect('이 문장은 한글로 쓰여졌습니다.').lang\n",
    "lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = translator.translate('이 문장은 한글로 쓰여졌습니다.',\n",
    "                           src=lang,\n",
    "                           dest='en')\n",
    "eng.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Exploring a Corpus\n",
    "##########\n",
    "df1 = df1[['state','snippet']]\n",
    "# Number of documents\n",
    "len(df1['snippet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of label categories (e.g. states)\n",
    "df1['state'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples per class\n",
    "df1['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words per sample\n",
    "def get_words_per_sample(txt):\n",
    "    return len(txt.split())\n",
    "df1['num_words'] = df1['snippet'].apply(get_words_per_sample)\n",
    "df1['num_words'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency distribution over words\n",
    "from collections import Counter\n",
    "freqs = Counter()\n",
    "for i, row in df1.iterrows():\n",
    "    freqs.update(row['snippet'].lower().split())\n",
    "freqs.most_common()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Number of samples) / number of words per sample)\n",
    "len(df1['snippet']) / df1['num_words'].mean()\n",
    "# if this is above 1500, we will use the sequence representation recommended by Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "polarity = sid.polarity_scores(text)\n",
    "polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = df1.sample(frac=.2)\n",
    "def get_sentiment(snippet):\n",
    "    return sid.polarity_scores(snippet)['compound']\n",
    "dfs['sentiment'] = dfs['snippet'].apply(get_sentiment)\n",
    "dfs.sort_values('sentiment',inplace=True)\n",
    "list(dfs[:2]['snippet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
